{"cells":[{"cell_type":"markdown","source":["# Writing Continuous Applications with Structured Streaming Python APIs in Apache Spark\n\nTutorial for <img src=\"https://databricks.com/wp-content/uploads/2018/12/pydata-logo-4.png\" alt=\"\" width=\"6%\"/> Miami \n\nAt first glance, building a distributed streaming engine might seem as simple as launching a set of servers and pushing data between them. Unfortunately, distributed stream processing runs into multiple complications that don’t affect simpler computations like batch jobs. Fortunately, PySpark 2.4 and Databricks makes this simple!\n\nThis notebook shows how one can train a model using Apache Spark and MLlib then deploy that model using Spark's structured streaming for making predictions as a continunous application.\n\nThis example will use a credit card fraud use case to demonstrate how MLlib models and structured streaming can be combined, to constitutue a continunous application. In our hypothetical use case, we have some historical data of credit card transactions, some of which have been identified as fraud. We want to train a model using this historical data that can flag potentially fraudulent transactions coming in as a live stream. We then want to deploy that model as part of a data pipeline which will work with a stream of transaction data to identify potential fraud hotspots in a continunous manner."],"metadata":{}},{"cell_type":"markdown","source":["-sandbox\n<div style=\"line-height: 0; padding-top: 9px;\">\n  <img src=\"https://databricks.com/wp-content/uploads/2018/04/PySparkStructuredStreaming-1.jpg\" alt=\"Structrured Streaming\" width=\"50%\" style=>\n</div>"],"metadata":{}},{"cell_type":"markdown","source":["The dataset has been collected and analysed during a research collaboration of Worldline and the Machine Learning Group (http://mlg.ulb.ac.be) of ULB (Université Libre de Bruxelles) on big data mining and fraud detection. More details on current and past projects on related topics are available on http://mlg.ulb.ac.be/BruFence and http://mlg.ulb.ac.be/ARTML\n\nPlease cite: Andrea Dal Pozzolo, Olivier Caelen, Reid A. Johnson and Gianluca Bontempi. Calibrating Probability with Undersampling for Unbalanced Classification. In Symposium on Computational Intelligence and Data Mining (CIDM), IEEE, 2015\n\nA copy of this data and its licence are available at https://s3-us-west-2.amazonaws.com/ml-team-public-read/credit-card-fraud.zip"],"metadata":{}},{"cell_type":"markdown","source":["This dataset has 3 columns we'll be using.\n\n**pcaVector:** The PCA transformation of raw transaction data. The main idea of principal component analysis (PCA) is to reduce the dimensionality of a data set consisting of many variables correlated with each other. Put simply, it is a method of summarizing data.\n\n**amountRange:** This column is a value between 0 and 7 and tells us the approximate amount of a transaction. The values correspond to 0-1, 1-5, 5-10, 10-20, 20-50, 50-100, 100-200, and 200+ in dollars.\n\n**label:** 0 or 1, whether a transaction was fraudulent.\n\nWe want to build a model which will predict the label using the pcaVector and amountRange data. We'll do this by using a ML pipeline with 3 stages:\n* 1) A **OneHotEncoder** to build a vector from our _amountRange_ column. It is a process by which categorical variables are converted into a vector form that could be provided to ML algorithms to do a better job in prediction.\n* 2) A **Vector assembler** to merge our _pcaVector_ & _amountRange_ vector into our features vector. It is a transformer that combines a given list of columns into a single vector column. It is useful for combining raw features and features generated by different feature transformers into a single feature vector, in order to train ML models like logistic regression and decision trees \n* 3) A **GBTClassifier** to serve as our Estimator. It's a learning algorithm for classification. It supports binary labels, as well as both continuous and categorical features."],"metadata":{}},{"cell_type":"markdown","source":["## Setup input and output files"],"metadata":{}},{"cell_type":"code","source":["input_data = \"/databricks-datasets/credit-card-fraud/data\"\noutput_test_parquet_data = \"/tmp/pydata/credit-card-frauld-test-data\""],"metadata":{},"outputs":[],"execution_count":6},{"cell_type":"code","source":["#\n# Lets take a look at the schema of the historical dataset we'll be working with today\n#\ndata = spark.read.parquet(input_data)\ndisplay(data)"],"metadata":{},"outputs":[],"execution_count":7},{"cell_type":"code","source":["data.count()"],"metadata":{},"outputs":[],"execution_count":8},{"cell_type":"markdown","source":["We using PySpark so import the appropriate classes"],"metadata":{}},{"cell_type":"code","source":["from pyspark.ml.feature import OneHotEncoderEstimator, VectorAssembler, VectorSizeHint\nfrom pyspark.ml.classification import GBTClassifier\n\nfrom pyspark.sql.types import *\nfrom pyspark.sql.functions import count, rand, collect_list, explode, struct, count"],"metadata":{},"outputs":[],"execution_count":10},{"cell_type":"markdown","source":["The way we do this will be very familiar to anyone who has used MLlib, but because we intend to use this model in a streaming context, there a few things we should be aware of.\n\nFirst, you may notice that we used a `OneHotEncoderEstimator`, which is new in Spark 2.3, and not a OneHotEncoder, which has now been deprecated. This new estimator fixes several issues related of the `OneHotEncoder` and will also allow you to do one hot encoding on streaming dataframes.\n\nAnd the second thing to be aware of when using MLlib with structured streaming is that `VectorAssembler` has some limitations in a streaming context. Specifically, `VectorAssembler` can only work on Vector columns of known size. To address this issue we can explicitly specify the size of the pcaVector column so that we'll be be able to use our pipeline with structured streaming. To do this we'll use the `VectorSizeHint` transformer."],"metadata":{}},{"cell_type":"code","source":["oneHot = OneHotEncoderEstimator(inputCols=[\"amountRange\"], outputCols=[\"amountVect\"])\n\nvectorAssembler = VectorAssembler(inputCols=[\"amountVect\", \"pcaVector\"], outputCol=\"features\")\n\nestimator = GBTClassifier(labelCol=\"label\", featuresCol=\"features\")"],"metadata":{},"outputs":[],"execution_count":12},{"cell_type":"code","source":["from pyspark.ml.feature import VectorSizeHint\n\nvectorSizeHint = VectorSizeHint(inputCol=\"pcaVector\", size=28)"],"metadata":{},"outputs":[],"execution_count":13},{"cell_type":"markdown","source":["### Now we're ready to build a our ML Pipeline and fit it."],"metadata":{}},{"cell_type":"code","source":["from pyspark.ml import Pipeline\nfrom pyspark.sql.functions import col\n\npipeline = Pipeline(stages=[oneHot, vectorSizeHint, vectorAssembler, estimator])\n#\n# let's split the data into testing and training datasets. \n# We will shave the test dataset for later\n#\n#\ntrain = data.filter(col(\"time\") % 10 < 8)\ntest = data.filter(col(\"time\") % 10 >= 8)\n#\n# save our data into partitions so we can read them as files\n#\n(test.repartition(20).write\n  .mode(\"overwrite\")\n  .parquet(output_test_parquet_data))"],"metadata":{},"outputs":[],"execution_count":15},{"cell_type":"code","source":["train.count()"],"metadata":{},"outputs":[],"execution_count":16},{"cell_type":"code","source":["test.count()"],"metadata":{},"outputs":[],"execution_count":17},{"cell_type":"markdown","source":["## Let's fit the model with our training data"],"metadata":{}},{"cell_type":"code","source":["pipelineModel = pipeline.fit(train)"],"metadata":{},"outputs":[],"execution_count":19},{"cell_type":"markdown","source":["We can simulate a stream by reading our test data from a file, since we don't have a Kafka cluster availale for the demo.\nBut the effect is no different; you are still using PySpark APIs to read off the filesystem as you would off Kafka topics.\n\nFirst, let's define the schema"],"metadata":{}},{"cell_type":"code","source":["from pyspark.sql.types import *\nfrom pyspark.ml.linalg import VectorUDT\n\nschema = (StructType([StructField(\"time\", IntegerType(), True), \n                      StructField(\"amountRange\", IntegerType(), True), \n                      StructField(\"label\", IntegerType(), True), \n                      StructField(\"pcaVector\", VectorUDT(), True)]))"],"metadata":{},"outputs":[],"execution_count":21},{"cell_type":"markdown","source":["## **Start:** \nRead files simulating as a Kafka stream using one file at a time"],"metadata":{}},{"cell_type":"code","source":["streamingData = (spark.readStream \n                 .schema(schema) \n                 .option(\"maxFilesPerTrigger\", 1) \n                 .parquet(output_test_parquet_data)) # our test data"],"metadata":{},"outputs":[],"execution_count":23},{"cell_type":"markdown","source":["Transform the Streaming DataFrame using the model and use DataFrame PySpark API to make queries"],"metadata":{}},{"cell_type":"code","source":["from pyspark.sql.functions import *\n\nstream = pipelineModel.transform(streamingData)"],"metadata":{},"outputs":[],"execution_count":25},{"cell_type":"markdown","source":["## Do aggregations using PySpark DataFrame APIs\n\n1. _groupBy_(\"label\", \"preditcions\")\n2. _sort_(\"label\", \"predictions\")\n\nAnd finally _display()_ the predictions as they are scored in real-time from the stream"],"metadata":{}},{"cell_type":"code","source":["streamPredictions = (pipelineModel.transform(streamingData) #infer or score against our test data\n          .groupBy(\"label\", \"prediction\")\n          .count()\n          .sort(\"label\", \"prediction\"))"],"metadata":{},"outputs":[],"execution_count":27},{"cell_type":"code","source":["display(streamPredictions)"],"metadata":{},"outputs":[],"execution_count":28},{"cell_type":"markdown","source":["### HOME WORK CHALLENGE-1:\nCan you compute the Precision and Recall?\n\nNote: The formula to compute Precision (P) and Recall (r)\n\n<div style=\"text-align: center; line-height: 0; padding-top: 9px;\">\n  <img src=\"https://databricks.com/wp-content/uploads/2019/01/Screen-Shot-2019-01-02-at-9.38.02-AM.png\" alt=\"Structrured Streaming\" width=\"20%\" style=>\n</div>"],"metadata":{}},{"cell_type":"markdown","source":["This notebook demonstrates that MLlib Transformers, including PipelineModels, can be applied to streaming DataFrames. Except for the minor differences described in this notebook, you can work with structured streaming DataFrames the same way you would with batch DataFrames using MLlib.\n\nThat's the beauty of unified apis in Apache Spark 2.x and the ability to work with streaming DataFrames as static ones. \n\nVery cool!"],"metadata":{}},{"cell_type":"markdown","source":["### HOME WORK CHALLENGE-2:\nCan you compute the F1 score, after computing the Precision and Recall?\n\n**Note**: F1 Score = 2(PR/P + R)"],"metadata":{}},{"cell_type":"code","source":["dbutils.fs.rm(output_test_parquet_data, True)"],"metadata":{},"outputs":[],"execution_count":32}],"metadata":{"name":"2-continuous_application_ml_serving","notebookId":1894321},"nbformat":4,"nbformat_minor":0}
