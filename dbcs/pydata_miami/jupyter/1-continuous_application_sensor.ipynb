{"cells":[{"cell_type":"markdown","source":["# Writing Continuous Applications with Structured Streaming Python APIs in Apache Spark\n\nTutorial for <img src=\"https://databricks.com/wp-content/uploads/2018/12/pydata-logo-4.png\" alt=\"\" width=\"6%\"/>  Miami \n\n\nAt first glance, building a distributed streaming engine might seem as simple as launching a set of servers and pushing data between them. Unfortunately, distributed stream processing runs into multiple complications that don’t affect simpler computations like batch jobs. Fortunately, PySpark 2.4 and Databricks makes this simple!\n\n**Orignal Author**: Michael John\n\nModified and Ported to PySpark by Jules S. Damji for the Tutorial"],"metadata":{}},{"cell_type":"markdown","source":["-sandbox\n<div style=\"line-height: 0; padding-top: 9px;\">\n  <img src=\"https://databricks.com/wp-content/uploads/2018/12/PySparkStructuredStreaming_FINAL.jpg\" alt=\"Structrured Streaming\" width=\"40%\" style=>\n</div>"],"metadata":{}},{"cell_type":"markdown","source":["## Setup Data"],"metadata":{}},{"cell_type":"code","source":["%run \"./setup/setup_data\""],"metadata":{},"outputs":[],"execution_count":4},{"cell_type":"markdown","source":["### PySpark Documentation: [Click here](https://spark.apache.org/docs/latest/api/python/index.html)\n * [DataFrame](https://spark.apache.org/docs/latest/api/python/pyspark.sql.html?highlight=dataframe#pyspark.sql.DataFrame)\n * [Spark SQL](https://spark.apache.org/docs/latest/api/python/pyspark.sql.html#module-pyspark.sql.types)\n * [Spark SQL Functions](https://spark.apache.org/docs/latest/api/python/pyspark.sql.html#module-pyspark.sql.functions)"],"metadata":{}},{"cell_type":"markdown","source":["# Processing Streaming Sensor Data\n\nStructured Streaming is a powerful capability for building end-to-end continuous applications. At a high-level, it offers the following features:\n\n1. __Output tables are always consistent__ with all the records in a prefix (partition) of the data, we will process and count in order.\n1. __Fault tolerance__ is handled holistically by Structured Streaming, including in interactions with output sinks.\n1. Ability to handle __late and out-of-order event-time data__. \n\n<img src=\"https://demo.cloud.databricks.com/files/mjohns/streaming/continuous-apps-1024x366.png\" alt=\"\" width=\"40%\"/>\n\n<sub>reference [Structured Streaming Blog](https://databricks.com/blog/2016/07/28/structured-streaming-in-apache-spark.html)</sub>"],"metadata":{}},{"cell_type":"markdown","source":["## 1-Sources\n\nConsider the input data stream as the “Input Table”. Every data item that is arriving on the stream is like a new row being appended to the Input Table.\n\n<img src=\"https://demo.cloud.databricks.com/files/mjohns/streaming/cloudtrail-unbounded-tables.png\" alt=\"\" width=\"40%\"/>"],"metadata":{}},{"cell_type":"markdown","source":["## 2-Continuous Processing & Queries\n\nThe developer then defines a query on this source, or input table, _as if it were a static table_ to compute a final result table that will be written to an output sink. Spark automatically converts this batch-like query to a streaming execution plan. This is called incrementalization: Spark figures out what state needs to be maintained to update the result each time a record arrives. Finally, developers specify triggers to control when to update the results. Each time a trigger fires, Spark checks for new data (new row in the input table), and incrementally updates the result.\n\n<img src=\"https://demo.cloud.databricks.com/files/mjohns/streaming/cloudtrail-structured-streaming-model.png\" alt=\"\" width=\"40%\"/>"],"metadata":{}},{"cell_type":"markdown","source":["## 3-Sinks\n\nThe last part of the model is output modes. Each time the result table is updated, the developer wants to write the changes to an external system, such as S3, HDFS, or a database. We usually want to write output incrementally. For this purpose, Structured Streaming provides three output modes:\n\n* __Append__: Only the new rows appended to the result table since the last trigger will be written to the external storage. \n* __Complete__: The entire updated result table will be written to external storage, e.g. for aggregates.\n* __Update__: Only the rows that were updated in the result table since the last trigger will be changed in the external storage. \n\n<img src=\"https://demo.cloud.databricks.com/files/mjohns/streaming/stream-example1-phone-updated.png\" width=\"40%\"/>"],"metadata":{}},{"cell_type":"markdown","source":["## A Continuous Application Example using PySpark Structured Streaming APIs"],"metadata":{}},{"cell_type":"markdown","source":["Setup some file paths our S3 bucket for output, checkpoint, and bad records"],"metadata":{}},{"cell_type":"code","source":["output_path = \"/tmp/pydata/Streaming/continuous_streaming/out/iot-stream/\"\ncheckpoint_path = \"/tmp/pydata/Streaming/continuous_streaming/out/iot-stream-checkpoint\"\n#\n#create checkpoint  path\n#\ndbutils.fs.rm(checkpoint_path,True) #overwrite checkpoint\ndbutils.fs.mkdirs(checkpoint_path)\n#\n#\nbad_records_path = \"/tmp/pydata/Streaming/continuous_streaming/badRecordsPath/streaming-sensor/\"\ndbutils.fs.rm(bad_records_path, True) #empty dir\ndbutils.fs.mkdirs(bad_records_path)"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"><span class=\"ansired\">Out[</span><span class=\"ansired\">2</span><span class=\"ansired\">]: </span>True\n</div>"]}}],"execution_count":12},{"cell_type":"markdown","source":["### What does the data from the sensors look like?"],"metadata":{}},{"cell_type":"code","source":["sensor_path = \"/mnt/jules-pydata/Streaming/continuous_streaming/streaming_sensor/\"\nsensor_file_name= sensor_path + \"streaming-sensor_file-1.json\"\ndbutils.fs.head(sensor_file_name, 233)"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">[Truncated to first 233 bytes]\n<span class=\"ansired\">Out[</span><span class=\"ansired\">3</span><span class=\"ansired\">]: </span>&apos;{&quot;timestamp&quot;:&quot;2016-08-03 01:32:20.454&quot;,&quot;deviceId&quot;:44,&quot;deviceType&quot;:&quot;SensorTypeD&quot;,&quot;signalStrength&quot;:0.6627327869584749}\\n{&quot;timestamp&quot;:&quot;2016-08-03 01:32:30.579&quot;,&quot;deviceId&quot;:63,&quot;deviceType&quot;:&quot;SensorTypeB&quot;,&quot;signalStrength&quot;:0.6419303214810813}&apos;\n</div>"]}}],"execution_count":14},{"cell_type":"markdown","source":["### Define schemas for incoming stream and outgoing stream\n\nGood best practice to define a schema and not have Spark infer it, for performance reasons. \nWithout a schema, Spark will launch couple of jobs: one to read header, and another to read\na good chuck of the partition to validate the schema, ensuring it matches the data.\n\nNow there are options that you can specifiy so that it fails fast, is tolerant, or subsitute missing\n values or mismatch dataytpes with NaN or null."],"metadata":{}},{"cell_type":"code","source":["from pyspark.sql.functions import *\nfrom pyspark.sql.types import *\n\n#original input schema\njsonSchema = (\n  StructType()\n  .add(\"timestamp\", TimestampType()) #event time at the source\n  .add(\"deviceId\", LongType())\n  .add(\"deviceType\", StringType())\n  .add(\"signalStrength\", DoubleType())\n)\n# modified schema with added columns since we are \n# doing some ETL (transforming and adding extra columns)\n# this transformed data will be stored into parquet files\n# from which an SQL table can be created for consumption or\n# report generation\nparquetSchema = (\n  StructType()\n  .add(\"timestamp\", TimestampType()) #event time at the source\n  .add(\"deviceId\", LongType())\n  .add(\"deviceType\", StringType())\n  .add(\"signalStrength\", DoubleType())\n  .add(\"INPUT_FILE_NAME\", StringType()) #file name from which this data item was read\n  .add(\"PROCESSED_TIME\", TimestampType())) #time at the executor while processing"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":16},{"cell_type":"markdown","source":["### Read Stream from Object Store Source\n\nIn this case we're simulating a Kafka live stream by reading in a file at a time. But this could as well be Apache Kafka topics\n\n__Notice: Intentionally slowing this down for tutorial.__"],"metadata":{}},{"cell_type":"code","source":["inputDF = ( spark \n          .readStream \n          .schema(jsonSchema) \n          .option(\"maxFilesPerTrigger\", 1)  #slow it down for tutorial\n          .option(\"badRecordsPath\", bad_records_path) #any bad records will go here\n          .json(sensor_path) #the source\n          .withColumn(\"INPUT_FILE_NAME\", input_file_name()) #maintain file path\n          .withColumn(\"PROCESSED_TIME\", current_timestamp()) #add a processing timestamp at the time of processing\n          .withWatermark(\"PROCESSED_TIME\", \"1 minute\") #optional: window for out of order data\n         )"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":18},{"cell_type":"markdown","source":["### Write Stream to Parquet File Sink"],"metadata":{}},{"cell_type":"code","source":["query = (inputDF\n         .writeStream\n         .format(\"parquet\") #our sink to save it for posterity or batch queries if needed\n         .option(\"path\", output_path)\n         .option(\"checkpointLocation\", checkpoint_path) # add checkpointing for resiliency\n         .outputMode(\"append\")\n         .queryName(\"devices\") #optionally a query name over write to issue queries against\n         .trigger(processingTime='5 seconds')\n         .start() \n        )"],"metadata":{},"outputs":[],"execution_count":20},{"cell_type":"code","source":["%fs ls /tmp/pydata/Streaming/continuous_streaming/out/iot-stream/"],"metadata":{},"outputs":[],"execution_count":21},{"cell_type":"markdown","source":["#### Create a temporary table from the input stream so that you can quickly issue SQL queries against it."],"metadata":{}},{"cell_type":"code","source":["inputDF.createOrReplaceTempView(\"parquet_sensors\")"],"metadata":{},"outputs":[],"execution_count":23},{"cell_type":"markdown","source":["### Run queries againt the temp table created off the input stream"],"metadata":{}},{"cell_type":"code","source":["%sql select * from parquet_sensors where deviceType = 'SensorTypeD' or deviceType = 'SensorTypeA'"],"metadata":{},"outputs":[],"execution_count":25},{"cell_type":"markdown","source":["(Click Run All Above Here)\n\n(Then Individually run below)"],"metadata":{}},{"cell_type":"markdown","source":["### Run queries and additional processing against Parquet file stored from the input stream"],"metadata":{}},{"cell_type":"code","source":["spark.conf.set(\"spark.sql.shuffle.partitions\", \"1\") #keep the size of shuffles small for better query performance\ndevices = (spark.readStream\n           .schema(parquetSchema)\n           .format(\"parquet\")\n           .option(\"maxFilesPerTrigger\", 1) #slow it down to demo\n           .load(output_path)\n           .withWatermark(\"PROCESSED_TIME\", \"1 minute\") #window for out of order data\n          )\n\n # generate temp table for more complex aggregation queries\ndevices.createOrReplaceTempView(\"sensors\")"],"metadata":{},"outputs":[],"execution_count":28},{"cell_type":"markdown","source":["#### What files are being processed?"],"metadata":{}},{"cell_type":"code","source":["display(\n  devices.\n  select(\"INPUT_FILE_NAME\", \"PROCESSED_TIME\")\n  .groupBy(\"INPUT_FILE_NAME\", \"PROCESSED_TIME\")\n  .count()\n  .orderBy(\"PROCESSED_TIME\", ascending=False)\n)"],"metadata":{},"outputs":[],"execution_count":30},{"cell_type":"markdown","source":["#### How much data is coming through?"],"metadata":{}},{"cell_type":"code","source":["%sql select count(*) from sensors"],"metadata":{},"outputs":[],"execution_count":32},{"cell_type":"markdown","source":["#### What does the min, max, and average strength look like for each sensor type?\n\nUse Spark SQL functions min(), max(), and avg()\n\nNote: I can use SQL within my Python notebook via `%sql` magic command"],"metadata":{}},{"cell_type":"code","source":["%sql \n\nselect count(*), deviceType, min(signalStrength), max(signalStrength), avg(signalStrength) \n  from sensors \n    group by deviceType \n    order by deviceType asc"],"metadata":{},"outputs":[],"execution_count":34},{"cell_type":"markdown","source":["#### Let's create a stream to aggregate signal counts by device and timestamp over 5 second windows.\n\nNote: this is tumbling window, not a sliding window; its size is 5 seconds\n\nFor example, discounting day, hours, and min, a tumbling window of size 5 looks as follows:\n\n*[(00:00 - 00:05), (00:05: 00:10), (00:10: 00:15)]*\n\nAn event could fall into any of these tumbling windows."],"metadata":{}},{"cell_type":"code","source":["(devices\n .groupBy(\n   window(\"timestamp\", \"5 seconds\"),\n   \"deviceId\"\n )\n .count()\n .createOrReplaceTempView(\"sensor_counts\")) #create a temporary view atop DataFrame"],"metadata":{},"outputs":[],"execution_count":36},{"cell_type":"markdown","source":["#### Which devices are experiencing signal loss over those 5 second windows?"],"metadata":{}},{"cell_type":"code","source":["%sql select * from sensor_counts where count < 5 order by window.start desc"],"metadata":{},"outputs":[],"execution_count":38},{"cell_type":"code","source":["// --- SHOW DASHBOARD ---"],"metadata":{},"outputs":[],"execution_count":39},{"cell_type":"markdown","source":["### Send Alerts for potentially down sensors that have not sent signals\n\nLet's create a DataFrame from our temporary table `sensor_counts`"],"metadata":{}},{"cell_type":"code","source":["lost_sensor_signals = (spark.table(\"sensor_counts\")\n         .filter(col(\"count\") < 5)\n         .select(\"window.start\", \"window.end\", \"deviceId\", \"count\")\n         )\n\n#display our DataFrame\ndisplay(lost_sensor_signals)"],"metadata":{},"outputs":[],"execution_count":41},{"cell_type":"markdown","source":["##### Using `foreach` mechanism to write to workers' logs.\n\nThis could be used for monitoring purposes. Either another job scanning the logs for ALERTS\nand publishing them onto a topic on Kafka or posting them on Ganglia. A good excerice to try\nif you have a Kafka cluster aviable or Ganglia service available via a REST API."],"metadata":{}},{"cell_type":"code","source":["def processRow(row):\n  # for now write them to log files, but this logic can easily be extended to publishing alerts\n  # to a topic on Kafka or monitoring/paging service such as Ganglia or PagerDuty.\n  print(\"ALERT from Sensors: Between {} and {}, device {} reported only {} times\".format(row.start, row.end, row.deviceId, row[3]))\n  \n(lost_sensor_signals\n .writeStream\n .outputMode(\"complete\") #could be our Kafka topic \"alerts\" for monitoring\n .foreach(processRow)\n .start()\n)"],"metadata":{},"outputs":[],"execution_count":43},{"cell_type":"markdown","source":["_Here is an example of the results being written out._\n\n<img src=\"https://databricks.com/wp-content/uploads/2018/12/Screen-Shot-2018-12-18-at-5.53.39-PM.png\" alt=\"\" width=\"50%\"/>"],"metadata":{}},{"cell_type":"markdown","source":["##### CHALLENGE-1: \n\nTry using`foreachBatch` to write each micro-batch using standard DataFrame API.\n\n__See [docs](https://docs.databricks.com/spark/latest/structured-streaming/foreach.html).__"],"metadata":{}},{"cell_type":"markdown","source":["##### CHALLENGE-2: \n\nTry adding adding additional monitoring conditions on devices. For example, add another streaming query that\nfilters monitoring conditions (signal strength < 0.2) from any of the above streams or tables: sensor_counts, devices, or inputStream, or parquet files.\n\n__See [docs](https://docs.databricks.com/spark/latest/structured-streaming/foreach.html).__"],"metadata":{}},{"cell_type":"markdown","source":["## Cleanup Data"],"metadata":{}},{"cell_type":"code","source":["%run \"./setup/cleanup_data\""],"metadata":{},"outputs":[],"execution_count":48}],"metadata":{"name":"1-continuous_application_sensor","notebookId":1894608},"nbformat":4,"nbformat_minor":0}
