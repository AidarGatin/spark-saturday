{"cells":[{"cell_type":"markdown","source":["# Writing Continuous Applications with Structured Streaming Python APIs in Apache Spark\n\nTutorial for <img src=\"https://databricks.com/wp-content/uploads/2018/12/pydata-logo-4.png\" alt=\"\" width=\"6%\"/> Miami"],"metadata":{}},{"cell_type":"markdown","source":["---\ntitle: Continuous Streaming - Event-time Aggregation and Watermarking in Structured Streaming\nauthors:\n- Michael Johns\n- Modified by Jules Damji (for PyData Miami)\n\ncreated_at: 2018-10-02\nupdated_at: 2019-1-5\n\ntldr: Demonstrates event-time aggregation, watermarks, windows, late data handling\n---"],"metadata":{}},{"cell_type":"markdown","source":["# Event-time Aggregation and Watermarking in Structured Streaming\n\nContinuous applications often require near real-time decisions on real-time aggregated statistics—such as health of and readings from IoT devices or detecting anomalous behavior. In this notebook, we will briefly explore how easily streaming aggregations can be expressed in Structured Streaming, and how naturally late, and out-of-order data is handled.\n\n### Stateful Incremental Execution \n\nStructured Streaming allows users to express the same streaming query as a batch query, and the Spark SQL engine incrementalizes the query and executes on streaming data. \n* Spark SQL engine internally maintains the intermediate aggregations as fault-tolerant state.\n* At every trigger, the state is read and updated in the state store, and all updates are saved to the write ahead log.\n\n<img src=\"https://demo.cloud.databricks.com/files/mjohns/streaming/watermarking/fault-tolerant-exactly-once-stateful-stream-processing-in-structured-streaming.png\" width=\"40%\"/>\n\n\n<sub>Reference [Blog Event-time Aggregation and Watermarking](https://databricks.com/blog/2017/05/08/event-time-aggregation-watermarking-apache-sparks-structured-streaming.html)</sub>\n\n<sub>Another [good reference](http://vishnuviswanath.com/spark_structured_streaming.html) to understand Sliding vs Tumbling Windows and Watermarking</sub>"],"metadata":{}},{"cell_type":"markdown","source":["## Setup"],"metadata":{}},{"cell_type":"code","source":["%run \"./setup/setup_data\""],"metadata":{},"outputs":[],"execution_count":5},{"cell_type":"markdown","source":["### Input path for sensors."],"metadata":{}},{"cell_type":"code","source":["sensor_path = \"/mnt/jules-pydata/Streaming/continuous_streaming/streaming_sensor/\"\ndbutils.fs.head(\"{}streaming-sensor_file-1.json\".format(sensor_path), 233)"],"metadata":{},"outputs":[],"execution_count":7},{"cell_type":"code","source":["from pyspark.sql.functions import *\nfrom pyspark.sql.types import *\n\njsonSchema = (\n  StructType()\n  .add(\"timestamp\", TimestampType())\n  .add(\"deviceId\", LongType())\n  .add(\"deviceType\", StringType())\n  .add(\"signalStrength\", DoubleType())\n)"],"metadata":{},"outputs":[],"execution_count":8},{"cell_type":"code","source":["# DataFrame w/ schema [eventTime: timestamp, deviceId: string, signal: bigint]\neventsDF = (\n  spark\n    .readStream\n    .schema(jsonSchema)\n      .option(\"maxFilesPerTrigger\", 1) # slow it down to demo\n    .json(sensor_path) # the source\n)"],"metadata":{},"outputs":[],"execution_count":9},{"cell_type":"markdown","source":["## Standard Aggregations (not Windowed)\n\nLet's do some normal standard aggregations such as average signal strengh and groupBy deviceid."],"metadata":{}},{"cell_type":"code","source":["display(eventsDF.groupBy(\"deviceId\").avg(\"signalStrength\"))"],"metadata":{},"outputs":[],"execution_count":11},{"cell_type":"markdown","source":["## Aggregations on Windows over Event-Time\n\nIn many cases, rather than running aggregations over the whole stream, you want aggregations over data bucketed by time windows (say, every 5 minutes or every hour), e.g. see what is the average signal strength in last 5 minutes in case if the devices have started to behave anomalously (example below). \n* Move beyond just _processing-time_ windows (when data hits the system)\n* Handle _event-time_ windows (when events actually happened, reflected in a field in the data itself) \n\n<img src=\"https://demo.cloud.databricks.com/files/mjohns/streaming/watermarking/mapping-of-event-time-to-5-min-tumbling-windows.png\" width=\"40%\"/>\n\n__Notice each window is a group for which running counts are calculated.__"],"metadata":{}},{"cell_type":"code","source":["display(\n  eventsDF \n    .groupBy(window(\"timestamp\", \"5 minute\")) \n    .count()\n)"],"metadata":{},"outputs":[],"execution_count":13},{"cell_type":"markdown","source":["You can also define overlapping windows by specifying both the window length and the sliding interval (example below).\n\n<img src=\"https://demo.cloud.databricks.com/files/mjohns/streaming/watermarking/mapping-of-event-time-to-overlapping-windows-of-length-10-mins-and-sliding-interval-5-mins.png\" width=\"40%\"/>\n\n__Notice this grouping strategy automatically handles _late_ and _out-of-order data_ — the late event would just update older window groups instead of the latest ones.__"],"metadata":{}},{"cell_type":"code","source":["display (\n  eventsDF\n    .groupBy(window(\"timestamp\", \"10 minutes\", \"5 minutes\"))\n    .count()\n)"],"metadata":{},"outputs":[],"execution_count":15},{"cell_type":"markdown","source":["### A complex end-to-end query"],"metadata":{}},{"cell_type":"markdown","source":["Here is an end-to-end illustration of a query that is grouped by both the `deviceId` and the overlapping windows. The illustration below shows how the final result of a query changes after new data is processed with 5 minute triggers when you are grouping by both deviceId and sliding windows.\n\n<img src=\"https://demo.cloud.databricks.com/files/mjohns/streaming/watermarking/late-data-handling-in-windowed-grouped-aggregation.png\" width=\"40%\"/>\n\n__Notice how the late, out-of-order record [12:04, dev2] updated an old window’s count.__"],"metadata":{}},{"cell_type":"code","source":["# Showing the groupBy 'deviceId'\ndisplay (\n  eventsDF \n    .groupBy(\n      \"deviceId\",\n      window(\"timestamp\", \"10 minutes\", \"5 minutes\")) \n    .count()\n    .withColumn(\n      \"window_size\", \n      (col(\"window.end\").cast(\"Long\") - col(\"window.start\").cast(\"Long\")) / 60\n    )\n    .orderBy(\"window.end\", ascending=False)\n)"],"metadata":{},"outputs":[],"execution_count":18},{"cell_type":"markdown","source":["## Watermarking to Limit State while Handling Late Data\nThe arrival of late data can result in updates to older windows. This complicates the process of defining which old aggregates are not going to be updated and therefore can be dropped from the state store to limit the state size. In Apache Spark 2.1+, watermarking enables automatic dropping of old state data.\n\n<img src=\"https://demo.cloud.databricks.com/files/mjohns/streaming/watermarking/watermarking-in-windowed-grouped-aggregation.png\" width=\"40%\"/>\n\n__Notice in the example above, a \"too late\" event arrives between the processing-times 12:20 and 12:25. The watermark is used to differentiate between late and the “too-late” events and treat them accordingly.__\n\nA much simpler examples that illusrates the concept behing watermark is [this blog](http://vishnuviswanath.com/spark_structured_streaming.html)\n\n<img src=\"https://databricks.com/wp-content/uploads/2019/01/watermarking_concept.png\" width=\"40%\"/>"],"metadata":{}},{"cell_type":"code","source":["# Same example as above, just showing `group by` for device with watermarking to define boundaries for \"too late\" data.\n\ndisplay (\n  eventsDF \n    .withWatermark(\"timestamp\", \"10 minutes\") \n    .groupBy(\n      \"deviceId\",\n      window(\"timestamp\", \"10 minutes\", \"5 minutes\")) \n    .count()\n    .withColumn(\n      \"window_size\", \n      (col(\"window.end\").cast(\"Long\") - col(\"window.start\").cast(\"Long\")) / 60\n    )\n    .orderBy(\"window.end\", ascending=False)\n)"],"metadata":{},"outputs":[],"execution_count":20},{"cell_type":"markdown","source":["_Structured Streaming’s windowing strategy handles key streaming aggregations: __windows over event-time and late and out-of-order data__. Using this windowing strategy allows Structured Streaming engine to implement watermarking, in which late data can be discarded. As a result of this design, we can manage the size of the state-store._"],"metadata":{}}],"metadata":{"name":"3_continuous_event_time_agg_watermarking","notebookId":1894575},"nbformat":4,"nbformat_minor":0}
